---
title: First Two Democratic Debates
author: Scott Frechette
date: '2019-08-01'
categories:
  - politics
tags:
  - R
  - politics
  - debates
  - text
slug: presidential-debates
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

Most Americans were smart enough to stay away from the two-night, four-hour marathon that was the second Democratic debate. I wasn't that smart, and like many others could use some help remembering what everyone talked about. 

Seems like a perfect time to boot up the [tidylo](https://github.com/juliasilge/tidylo) package again and try out a few more tricks. 

## Setup

Here's everything we need to load up to do this.

```{r}
library(tidyverse)
library(rvest)
library(lubridate)
library(janitor)
library(scales)
library(tidytext)
library(textdata)
library(tidylo)
library(umap)
library(ggrepel)
library(widyr)
library(igraph)
library(ggraph)

set.seed(42)

theme_set(tidyquant::theme_tq())
```

## Get Debate Data

First thing we need to do is scrape the last debate. Washington Post has the transcripts posted so we'll borrow those.

```{r eval=FALSE}
night1 <- read_html("https://www.washingtonpost.com/politics/2019/07/31/transcript-first-night-second-democratic-debate/?utm_term=.679169e787bc") %>%
  html_nodes("p") %>%
  html_text() %>%
  enframe(name = NULL) %>%
  slice(6:n()) %>%
  mutate(debate = "Democratic Debates: 2019/2020: Jul 30th",
         election_year = 2020)
 
night2 <- read_html("https://www.washingtonpost.com/politics/2019/08/01/transcript-night-second-democratic-debate/?utm_term=.679169e787bc") %>%
  html_nodes("p") %>%
  html_text() %>%
  enframe(name = NULL) %>%
  slice(5:n()) %>%
  mutate(debate = "Democratic Debates: 2019/2020: Jul 31st",
         election_year = 2020)
 
dem_2020_2 <- bind_rows(night1, night2) %>%
  rename(text = value) %>%
  mutate(speaker = str_extract(text, "^[A-Z'\\s]*:") %>%
           str_remove(":") %>%
           str_remove("^ELIZABETH|^KAMALA"),
         text = str_remove(text, "^[A-Z'\\s]*:") %>%
           str_remove_all("\\([A-Za-z-]*\\)") %>%
           str_trim()) %>%
  fill(speaker) %>%
  select(speaker, text, debate, election_year) %>%
  mutate(party = case_when(
    speaker %in% c("BASH", "TAPPER", "LEMON", "PROTESTOR") ~ "NON-CANDIDATE",
    TRUE ~ "DEMOCRAT"
  ))
 
```

Our old pal Tyler Schnoebelen* has gone through the effort to pull all the historical debates so why duplicate his effort? This gives us the first Democratic debate of 2020 and also lets us mess around with some historical data going back 60 years.  


&ast; Not actually friends

```{r eval=FALSE}
debates <- read_csv("https://raw.githubusercontent.com/TylerSchnoebelen/US_debates/master/All_American_Presidential_and_VP_debates_since_Kennedy-Nixon-1960.csv") %>%
  clean_names() %>%
  select(-new_id) %>%
  bind_rows(dem_2020_2) %>%
  mutate(text = str_remove_all(text, "\\([A-Za-z-]*\\)"),
         speaker = str_remove(speaker, "^[A-Z]* "),
         speaker = if_else(str_detect(speaker, "ROURKE"), "O'ROURKE", speaker)) %>%
  filter(text != "")

debates
```

```{r eval=FALSE, include=FALSE}
save(debates, file = here::here("static", "data", "2019-08-01_debates.rds"))
```


```{r echo=FALSE}
load(here::here("static", "data", "2019-08-01_debates.rds"))

debates
```


## Tokenize

Now to tokenize these debates. I've gone ahead and created unigrams and bigrams so we can play around with both. 

```{r}
debates_unigrams <- debates %>%
  filter(party %in% c("DEMOCRAT", "REPUBLICAN"),
         election_year == 2020,
         str_detect(debate, "Jul")) %>%
  unnest_tokens(word, text)

debates_bigrams <- debates %>%
  filter(party %in% c("DEMOCRAT", "REPUBLICAN"),
         election_year == 2020,
         str_detect(debate, "Jul")) %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

## Log Odds

Just like last time we're going to use the `tidylo` package to calculate the weighted log odds to figure out what tokens matter most for each of our candidates.

Let's start with the unigrams. Now that we've seen these candidates twice let's see if we get a feel so what matters more to them this year. We'll filter for words that a candidate uttered at least three times and we're relatively confident are important for them.

```{r fig.width=8, fig.height=8}
debates_unigrams %>%
  anti_join(stop_words, by = "word") %>%
  count(speaker, word) %>%
  bind_log_odds(speaker, word, n) %>%
  filter(log_odds >= 1.96,
         n >= 3) %>%
  group_by(speaker) %>% 
  slice(1:7) %>%
  ungroup() %>% 
  mutate(word = reorder_within(word, log_odds, speaker)) %>%
  ggplot(aes(word, log_odds, fill = n)) +
  geom_col() +
  scale_x_reordered() +
  scale_fill_viridis_c(guide = F) +
  coord_flip() +
  facet_wrap(~ speaker, scales = "free_y", ncol = 4)
```

What can we tell? Biden is defending Obamacare. Bullock is from Montana and doesn't like Koch brothers. Gillibrand is worried about the deterioration of women. Warren wants to fight giants.  

Frankly it's not that helpful. I still prefer bigrams so let's see if those do us any better. 

```{r fig.width=10, fig.height=10}
debates_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(speaker, bigram) %>%
  bind_log_odds(speaker, bigram, n) %>%
  arrange(-log_odds) %>% 
  filter(
    # log_odds >= 1.96,
    # n >= 3
    ) %>%
  group_by(speaker) %>% 
  slice(1:7) %>% 
  ungroup() %>% 
  mutate(bigram = reorder_within(bigram, log_odds, speaker)) %>%
  ggplot(aes(bigram, log_odds, fill = n)) +
  geom_col() +
  scale_x_reordered() +
  scale_fill_viridis_c(guide = F) +
  coord_flip() +
  facet_wrap(~ speaker, scales = "free_y", ncol = 4)
```

This looks a lot better. 

In fact, let's compare it to [CNN's own analysis of the second debate with tidytext](https://www.cnn.com/interactive/2019/08/politics/democratic-debate-key-phrases/). Which of CNN's key phrases identified with TF-IDF did we match? Let's compare their list to the top 10 unigrams and bigrams for each candidate by weighted log odds:

* Bennet: **universal healthcare**, **public option**, **divisive politics**
* Biden: **deeply involved**, **fundamentally change**, **obamacare**, hyde amendment
* De Blasio: **NAFTA**, **status quo**, wealthy, iran
* Booker: **civil courts**, **crime bill**, **common purpose**
* Bullock: **koch brothers**, **dark money**, citizens united
* Buttigieg: **structural reform**, **endless war**, courage, community
* Castro: **family separation**, **immigration plan**, begin impeachment
* Delaney: **impossible promises**, **real solutions**, **private sector**
* Gabbard: **cold war**, **deployed**, betrayed, **fair trade**
* Gillibrand: America's women, **deterioration**, avoiding responsibility
* Harris: **justice**, **civil rights**, **American families**, **death penalty**
* Hickenlooper: **track record**, **trade war**, **people choose**
* Inslee: **clean energy**, fossil fuels, **climate crisis**
* Klobuchar: **background checks**, border security, **bread and butter issue**
* O'Rourke: **electoral college**, human rights, **El Paso**
* Ryan: **manufacturing**, auto workers, **China**
* Sanders: drug companies, **fossil fuel**, **corporate America**, **transform**
* Warren: giant corporations, **insurance company**, **fight**, risk
* Williamson: **deep truth**, false god, **collectivized hazard**, heal
* Yang: **game changer**, automation, **manufacturing jobs**, **reality TV**

I'd say that's pretty damn good. And let's note they didn't publish their code in the article so hard to know how much human intuition was added to their final list. 

## UMAP

Another technique I like a lot is called [UMAP](https://arxiv.org/abs/1802.03426). It's a dimenstion reduction technique similar to PCA and t-SNE but I tend to think it works a little better. It's particularly helpful for visualizing high dimensional data. 

First can we apply UMAP to the words used by candidates to plot candidates in space near their neighbors?

```{r}
debates_unigrams_umap <- debates_unigrams %>%
  count(speaker, word) %>%
  bind_log_odds(speaker, word, n) %>% 
  cast_sparse(speaker, word, log_odds) %>% 
  as.matrix() %>% 
  umap()

debates_unigrams_umap$layout %>% 
  as_tibble(rownames = "candidate") %>% 
  ggplot(aes(V1, V2, label = candidate)) + 
  geom_point() + 
  geom_text_repel() + 
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_hline(yintercept = 0, linetype = 2)
```

To be honest I could probably squint hard and make out some patterns, but it looks generally random to my tiny human brain.

So what about reversing that and visualizing the words in space instead? I'll remove stop words and only focus on top 200 words to help the visual. 

```{r fig.width=12, fig.height=12}

top_words <- debates_unigrams %>% 
  count(word, sort = T) %>% 
  filter(str_detect(word, "^[A-Za-z]")) %>% 
  anti_join(stop_words, by = "word") %>% 
  top_n(200, n)

debates_unigrams %>%
  inner_join(top_words, by = "word") %>% 
  count(speaker, word) %>% 
  cast_sparse(word, speaker, n) %>% 
  as.matrix() %>% 
  umap() %>% 
  .$layout %>% 
  as_tibble(rownames = "word") %>% 
  ggplot(aes(V1, V2, label = word)) + 
  geom_point() + 
  geom_text_repel() + 
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_hline(yintercept = 0, linetype = 2)
```

Looks like some decent clusters in there we could use to help guide us in more unsupervised learning techniques like K-means clustering or topic modeling. 

Just for fun let's see what happens if I combine UMAP with our weighted log odds from above.

```{r fig.width=12, fig.height=12}
debates_unigrams %>%
  inner_join(top_words, by = "word") %>% 
  count(speaker, word) %>% 
  bind_log_odds(word, speaker, n) %>% 
  cast_sparse(word, speaker, log_odds) %>% 
  as.matrix() %>% 
  umap() %>% 
  .$layout %>% 
  as_tibble(rownames = "word") %>% 
  ggplot(aes(V1, V2, label = word)) + 
  geom_point() + 
  geom_text_repel() + 
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_hline(yintercept = 0, linetype = 2)
```

Looks a little bit better and shows the flexibility and power of using weighted log odds for any bag of words analysis. 

## Sentiment Analysis

Let's have a little more fun and do some sentiment analysis. Julia Silge has a [recent post](https://juliasilge.com/blog/sentiment-lexicons/) about correcting their mistakes with sentiment analysis lexicons. I'll leave it to the reader to check that out and download the updates yourselves. 

#### Bing

First up is the Bing lexicon, which characterizes words as positive or negative. Let's apply this to our text and find the rate of both positive and negative words for each candidate: 

```{r}
debates_unigrams_bing <- debates_unigrams %>% 
  add_count(speaker, name = "words") %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(speaker, sentiment, words, name = "sentiment_score") %>%
  mutate(score_adj = sentiment_score / words,
         score_adj_100 = score_adj * 100, 
         speaker = reorder_within(speaker, score_adj, sentiment))

debates_unigrams_bing %>% 
  ggplot(aes(speaker, score_adj_100, fill = sentiment == "positive")) +
  geom_col() +
  coord_flip() + 
  facet_wrap(~ sentiment, scales = "free_y") + 
  scale_x_reordered() + 
  guides(fill = F) + 
  labs(y = "Positive/Negative Score (per 100 words)",
       x = "Candidate")
```

Based on this Gabbard, Booker, and Swalwell are the Debbie Downers and Inslee and Hickenlooper are the optimists.

#### AFinn

What about trying out the AFINN lexicon? This goes a step further than Bing and assigns each word a sentiment score between -5 and 5. 

```{r}
debates_unigrams_afinn <- debates_unigrams %>% 
  add_count(speaker, name = "words") %>% 
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  count(speaker, words, wt = value, name = "sentiment_score") %>% 
  mutate(score_adj = sentiment_score / words,
         score_adj_100 = score_adj * 100) %>%
  arrange(-score_adj)

debates_unigrams_afinn %>% 
  ggplot(aes(reorder(speaker, score_adj), score_adj_100, fill = score_adj > 0)) +
  geom_col() + 
  coord_flip() + 
  guides(fill = F) +
  labs(y = "Adjusted Sentiment Score",
       x = "Candidate")
```

Swalwell and Gabbard are still negative but now Sanders has joined them. So maybe he says less negative words but they tend to be more dramatic, which feels reasonable to me. 

#### NRC

Finally let's check out the NRC lexicon. This categorizes words as positive/negative like Bing but also adds more categories for the eight basic emotions of anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

I'm filtering out positive/negative emotions because it's similar to what we did above. 

```{r}
debates_unigrams_nrc <- debates_unigrams %>% 
  add_count(speaker, name = "words") %>% 
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  filter(!sentiment %in% c("positive", "negative")) %>% 
  count(speaker, words, sentiment) %>% 
  mutate(score = n / words) %>% 
  arrange(-n)
```

How do the candidates compare for each sentiment? 

```{r fig.width=10, fig.height=10}
debates_unigrams_nrc %>% 
  mutate(speaker_ordered = reorder_within(speaker, score, sentiment)) %>% 
  ggplot(aes(speaker_ordered, score, fill = score)) +
  geom_col() + 
  scale_x_reordered() +
  scale_y_continuous(labels = percent) + 
  coord_flip() + 
  facet_wrap(~ sentiment, scales = "free_y") +
  guides(fill = F) +
  scale_fill_viridis_c()
```

Now we can actually see that Swalwell scored highest in fear and sadness whereas Gabbard scored highest in disgust. Meanwhile Inslee displayed a lot of joy and trust 

What if we transformed these scores into sentiment radar charts for our candidates? We'll scale each sentiment so we can truly assess each candidate against the others. 

```{r fig.width=8, fig.height=8}
debates_unigrams_nrc %>% 
  select(-n, -words) %>% 
  spread(sentiment, score) %>%
  mutate_at(vars(-speaker), rescale) %>% 
  gather(sentiment, score, -speaker) %>% 
  ggplot(aes(sentiment, score, group = 1, color = speaker)) + 
  geom_col(aes(fill = speaker)) + 
  coord_polar() + 
  facet_wrap(~ speaker) + 
  guides(color = FALSE, fill = FALSE) + 
  scale_y_continuous(labels = percent) + 
  theme(axis.title = element_blank())
```

Not too bad considering we didn't adjust for valence shifters (negations, intensifiers, or diminishers). Based on this maybe we should pay attention what Inslee has to say more and stay away from Gabbard and Buttigieg. 

Though really we know this is bullshit because it thinks the jewel herself, Marianne Williamson, is the angriest person onstage. 

## Pairwise Correlation

Let's try one more technique and see how it works out. Let's see if we can find decent clusters by identifying which words are highly correlated with each other. For this I'll go ahead and include words from all 2020 debates and lemmatize the words to reduce the dimensionality. 

```{r fig.width=12, fig.height=12}
debates_unigrams_2020 <- debates %>%
  filter(election_year == 2020,
         party == "DEMOCRAT") %>% 
  mutate(response = row_number()) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word") %>% 
  mutate(lemma = textstem::lemmatize_words(word)) %>% 
  count(speaker, response, lemma) %>% 
  add_count(lemma, wt = n, name = "total")

debates_unigrams_cor <- debates_unigrams_2020 %>% 
  filter(total > 5) %>% 
  pairwise_cor(lemma, response, n)

debates_unigrams_cor_filtered <- debates_unigrams_cor %>% 
  filter(correlation >= 0.35)

debates_unigrams_2020_count <- debates_unigrams_2020 %>% 
  filter(lemma %in% 
           debates_unigrams_cor_filtered$item1) %>%
  distinct(lemma, total) %>% 
  mutate(total = log2(total) + 3)

debates_unigrams_cor_filtered %>%
  graph_from_data_frame(vertices = debates_unigrams_2020_count) %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(aes(size = total),
                  color = "lightblue") + 
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void() + 
  guides(size = FALSE)
```

Most of this looks pretty reasonable, except it's a rough look that Eric Garner is connected to dominating the solar market. 

## Conclusion

My goal was to push `tidylo` package a little bit further to see what all we can do with it as well as try out some of the other techniques raised by Julia Silge and David Robinson in their book [Tidy Text Mining](https://www.tidytextmining.com/).

It'll be even better when I can just run future debates through this type of analysis and skip out on watching it entirely. 